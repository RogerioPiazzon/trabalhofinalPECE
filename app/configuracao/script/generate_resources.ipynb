{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\roger\\\\Documents\\\\trabalhofinalPECE\\\\app\\\\configuracao\\\\script'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_files_config():\n",
    "    bool_resources = True\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def get_info_column(self,file,sheet):\n",
    "        output = dict()\n",
    "        df_table = pd.read_excel(file,sheet_name=sheet[:31])\n",
    "        for i,row in df_table.iterrows():\n",
    "            output.update({row['Campo']:{\"Descrição\":row['Descrição']}})\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def create_dict_terms(self):\n",
    "        if not os.path.exists(r\"..\\..\\dados\\ddl\\create_tables.sql\") or not os.path.exists(r\"..\\rawfiles\\Metadados_preenchido.xlsx\") or not os.path.exists(r\"..\\..\\dados\\db\\dbo2.db\"): \n",
    "            print(\"Não foi possivel criar o arquivo solicitado\")\n",
    "            return None\n",
    "    \n",
    "        sql_file = open(r\"..\\..\\dados\\ddl\\create_tables.sql\",encoding=\"ISO-8859-1\")\n",
    "        sql_as_string = sql_file.read()\n",
    "        sql_file.close()\n",
    "        ddl_tables = dict()\n",
    "        relation_tables = dict()\n",
    "\n",
    "        regex_create_table = r\"\\bCREATE\\s+TABLE\\s+\\[\\w+\\]\\.\\[\\w+\\]\\s*\\(\\s*([^;]+)\\s*\\);\"\n",
    "        regex_name_table = r\"\\bCREATE\\s+TABLE\\s+\\[(\\w+)\\]\\.\\[(\\w+)\\]\"\n",
    "        regex_relations = r'FOREIGN KEY\\((\\w+)\\)\\s+REFERENCES\\s+(\\w+)\\((\\w+)\\)'\n",
    "\n",
    "        matches = re.finditer(regex_create_table, sql_as_string)\n",
    "\n",
    "        regex_name_table = r\"\\bCREATE\\s+TABLE\\s+\\[(\\w+)\\]\\.\\[(\\w+)\\]\"\n",
    "\n",
    "        for matche in matches:\n",
    "            list_relations = list()\n",
    "\n",
    "            name_table = re.search(regex_name_table, matche.group())\n",
    "            relations = re.finditer(regex_relations, matche.group())\n",
    "            for relation in relations:\n",
    "                list_relations.append([{name_table.group(2):relation.group(1)},{relation.group(2):relation.group(3)}])\n",
    "\n",
    "            \n",
    "            ddl_tables[name_table.group(2)]  = matche.group()\n",
    "            relation_tables[name_table.group(2)] = list_relations\n",
    "\n",
    "        df = pd.read_excel(r\"..\\rawfiles\\Metadados_preenchido.xlsx\",sheet_name=\"Resumo das tabelas\")\n",
    "\n",
    "        conn = sqlite3.connect(r\"..\\..\\dados\\db\\dbo2.db\")\n",
    "        metadata_json = dict()\n",
    "        for i,row in df.iterrows():\n",
    "            metadata_json.update({row[\"Nome\"]:{nm:vl if vl else \"\" for nm,vl in row.items() if nm != \"Nome\"} })\n",
    "            metadata_json[row[\"Nome\"]].update({\"Colunas\":self.get_info_column(r\"..\\rawfiles\\Metadados_preenchido.xlsx\",row[\"Nome\"])})\n",
    "            metadata_json[row[\"Nome\"]].update({\"DDL\":ddl_tables.get(row[\"Nome\"],'')})\n",
    "            metadata_json[row[\"Nome\"]].update({\"Relações\":relation_tables.get(row[\"Nome\"],'')})\n",
    "            sql_query = f\"\"\"SELECT * FROM {row['Nome']};\"\"\"\n",
    "            df_by_sql = pd.read_sql(sql_query,conn)\n",
    "            desc = df_by_sql.describe(include='all')\n",
    "            only_unique = desc.loc[desc.index.isin([\"unique\"])].T.reset_index()\n",
    "            lista_unique = only_unique.loc[only_unique['unique']<=5,'index'].values.tolist()\n",
    "            if len(lista_unique)>1:\n",
    "                for coluna in lista_unique:\n",
    "                    unique_by_col_sql = pd.read_sql(f\"\"\"SELECT DISTINCT {coluna} FROM {row[\"Nome\"]}\"\"\",conn)\n",
    "                    metadata_json[row[\"Nome\"]]['Colunas'][coluna].update({\"Valores Possíveis\":unique_by_col_sql[coluna].values.tolist()}) \n",
    "\n",
    "        with open(r\"..\\files\\dicionario_dados.json\", \"w\") as outfile: \n",
    "            json.dump(metadata_json, outfile)\n",
    "\n",
    "    def create_aventureqi(self):\n",
    "        if not os.path.exists(r\"..\\rawfiles\\querys.xlsx\"):\n",
    "            print(\"Não foi possivel criar o arquivo solicitado\")\n",
    "            return None\n",
    "\n",
    "        file = r\"..\\rawfiles\\querys.xlsx\"\n",
    "        df = pd.read_excel(file,sheet_name='Planilha1')\n",
    "        df_to_use = df.loc[:,[\"Question\",\"NEW INTENT\",\"SQL\"]].dropna()\n",
    "        data=df_to_use.rename(columns = {'NEW INTENT': 'INTENT', 'Question': 'QUESTION','SQL':'QUERY'}, inplace = False)\n",
    "        dict_output = dict()\n",
    "        for i,row in data.drop_duplicates().iterrows():\n",
    "            dict_output.update({row['QUESTION']:{'INTENT':row['INTENT'],'QUERY':row['QUERY']}})\n",
    "        with open(r\"..\\files\\AdventureQI.json\", \"w\") as outfile: \n",
    "            json.dump(dict_output, outfile)\n",
    "\n",
    "    def create_intent_table(self):\n",
    "        if not os.path.exists(r\"..\\rawfiles\\querys.xlsx\"):\n",
    "            print(\"Não foi possivel criar o arquivo solicitado\")\n",
    "            return None\n",
    "        dict_output = dict()\n",
    "        file = r\"..\\rawfiles\\querys.xlsx\"\n",
    "        df = pd.read_excel(file,sheet_name='Planilha1')\n",
    "        for i,row in df[[\"NEW INTENT\",\"TABLE\"]].drop_duplicates().dropna().iterrows():\n",
    "            dict_output.update({row['NEW INTENT']:[table.replace(' ','') for table in row['TABLE'].split(\",\")]})\n",
    "        \n",
    "        with open(r\"..\\files\\intent_table.json\", \"w\") as outfile: \n",
    "            json.dump(dict_output, outfile)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(pegar_cotacao_atual(\"EUR\", \"BRL\"))\n",
    "#     print(\"lira é doidao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testre = generate_files_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testre.create_dict_terms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando dicionario de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_file = open(r\"..\\..\\dados\\ddl\\create_tables.sql\",encoding=\"ISO-8859-1\")\n",
    "sql_as_string = sql_file.read()\n",
    "sql_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_tables = dict()\n",
    "relation_tables = dict()\n",
    "\n",
    "regex_create_table = r\"\\bCREATE\\s+TABLE\\s+\\[\\w+\\]\\.\\[\\w+\\]\\s*\\(\\s*([^;]+)\\s*\\);\"\n",
    "regex_name_table = r\"\\bCREATE\\s+TABLE\\s+\\[(\\w+)\\]\\.\\[(\\w+)\\]\"\n",
    "regex_relations = r'FOREIGN KEY\\((\\w+)\\)\\s+REFERENCES\\s+(\\w+)\\((\\w+)\\)'\n",
    "\n",
    "matches = re.finditer(regex_create_table, sql_as_string)\n",
    "\n",
    "regex_name_table = r\"\\bCREATE\\s+TABLE\\s+\\[(\\w+)\\]\\.\\[(\\w+)\\]\"\n",
    "\n",
    "for matche in matches:\n",
    "\tlist_relations = list()\n",
    "\n",
    "\tname_table = re.search(regex_name_table, matche.group())\n",
    "\trelations = re.finditer(regex_relations, matche.group())\n",
    "\tfor relation in relations:\n",
    "\t\tlist_relations.append([{name_table.group(2):relation.group(1)},{relation.group(2):relation.group(3)}])\n",
    "\n",
    "     \n",
    "\tddl_tables[name_table.group(2)]  = matche.group()\n",
    "\trelation_tables[name_table.group(2)] = list_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"..\\rawfiles\\Metadados_preenchido.xlsx\",sheet_name=\"Resumo das tabelas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_column(file,sheet):\n",
    "    output = dict()\n",
    "    df_table = pd.read_excel(file,sheet_name=sheet[:31])\n",
    "    for i,row in df_table.iterrows():\n",
    "        output.update({row['Campo']:{\"Descrição\":row['Descrição']}})\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(r\"..\\..\\dados\\db\\dbo2.db\")\n",
    "metadata_json = dict()\n",
    "for i,row in df.iterrows():\n",
    "    metadata_json.update({row[\"Nome\"]:{nm:vl if vl else \"\" for nm,vl in row.items() if nm != \"Nome\"} })\n",
    "    metadata_json[row[\"Nome\"]].update({\"Colunas\":get_info_column(file,row[\"Nome\"])})\n",
    "    metadata_json[row[\"Nome\"]].update({\"DDL\":ddl_tables.get(row[\"Nome\"],'')})\n",
    "    metadata_json[row[\"Nome\"]].update({\"Relações\":relation_tables.get(row[\"Nome\"],'')})\n",
    "\n",
    "    sql_query = f\"\"\"SELECT * FROM {row['Nome']};\"\"\"\n",
    "    df_by_sql = pd.read_sql(sql_query,conn)\n",
    "    desc = df_by_sql.describe(include='all')\n",
    "    only_unique = desc.loc[desc.index.isin([\"unique\"])].T.reset_index()\n",
    "    lista_unique = only_unique.loc[only_unique['unique']<=5,'index'].values.tolist()\n",
    "    if len(lista_unique)>1:\n",
    "        for coluna in lista_unique:\n",
    "            unique_by_col_sql = pd.read_sql(f\"\"\"SELECT DISTINCT {coluna} FROM {row[\"Nome\"]}\"\"\",conn)\n",
    "            metadata_json[row[\"Nome\"]]['Colunas'][coluna].update({\"Valores Possíveis\":unique_by_col_sql[coluna].values.tolist()}) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"..\\files\\dicionario_dados.json\", \"w\") as outfile: \n",
    "    json.dump(metadata_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando arquivo AdventureQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"..\\rawfiles\\querys.xlsx\"\n",
    "df = pd.read_excel(file,sheet_name='Planilha1')\n",
    "df_to_use = df.loc[:,[\"Question\",\"NEW INTENT\",\"SQL\"]].dropna()\n",
    "data=df_to_use.rename(columns = {'NEW INTENT': 'INTENT', 'Question': 'QUESTION','SQL':'QUERY'}, inplace = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_output = dict()\n",
    "for i,row in data.drop_duplicates().iterrows():\n",
    "    dict_output.update({row['QUESTION']:{'INTENT':row['INTENT'],'QUERY':row['QUERY']}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"..\\files\\AdventureQI.json\", \"w\") as outfile: \n",
    "    json.dump(dict_output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando tabela depara intenção x tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_output = dict()\n",
    "for i,row in df[[\"NEW INTENT\",\"TABLE\"]].drop_duplicates().dropna().iterrows():\n",
    "    dict_output.update({row['NEW INTENT']:[table.replace(' ','') for table in row['TABLE'].split(\",\")]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(r\"..\\files\\intent_table.json\", \"w\") as outfile: \n",
    "    json.dump(dict_output, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
