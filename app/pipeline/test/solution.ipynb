{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32731,"status":"ok","timestamp":1706054633046,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"TeZzzKGhLUAm","outputId":"356aad66-13e6-4296-eff5-8d2086accb42"},"outputs":[{"name":"stdout","output_type":"stream","text":["hqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\n"]}],"source":["#DO MIXTRAL\n","\n","import numpy\n","from IPython.display import clear_output\n","\n","# fix triton in colab\n","!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia\n","\n","!git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n","!cd mixtral-offloading && pip install -q -r requirements.txt\n","!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n","\n","!pip install maritalk -q\n","!pip install langchain[all] -q\n","\n","clear_output()\n","\n","import sys,os,glob,json,sqlite3,re\n","import pandas as pd\n","from datetime import datetime\n","import pickle\n","\n","sys.path.append(\"mixtral-offloading\")\n","\n","import torch\n","from torch.nn import functional as F\n","from hqq.core.quantize import BaseQuantizeConfig\n","from huggingface_hub import snapshot_download\n","from IPython.display import clear_output\n","from tqdm.auto import trange\n","from transformers import AutoConfig, AutoTokenizer, TFBertForSequenceClassification, TFXLMRobertaForSequenceClassification,TFDebertaV2ForSequenceClassification\n","from transformers.utils import logging as hf_logging\n","\n","from src.build_model import OffloadConfig, QuantConfig, build_model\n","\n","from typing import Any, List, Mapping, Optional\n","\n","from langchain.callbacks.manager import CallbackManagerForLLMRun\n","from langchain_core.language_models.llms import LLM\n","from langchain_core.language_models.chat_models import BaseChatModel, SimpleChatModel\n","from langchain_core.messages import (\n","    HumanMessage,\n","    SystemMessage,\n","    AIMessage,\n","    BaseMessage\n",")\n","from langchain.prompts.chat import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n","\n","import maritalk\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1706054633046,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"9IJkw8k__rLX"},"outputs":[],"source":["class SingletonMeta(type):\n","    \"\"\"\n","    The Singleton class can be implemented in different ways in Python. Some\n","    possible methods include: base class, decorator, metaclass. We will use the\n","    metaclass because it is best suited for this purpose.\n","    \"\"\"\n","\n","    _instances = {}\n","\n","    def __call__(cls, *args, **kwargs):\n","        \"\"\"\n","        Possible changes to the value of the `__init__` argument do not affect\n","        the returned instance.\n","        \"\"\"\n","        if cls not in cls._instances:\n","            instance = super().__call__(*args, **kwargs)\n","            cls._instances[cls] = instance\n","        return cls._instances[cls]"]},{"cell_type":"markdown","metadata":{"id":"szHoG8RG2NVi"},"source":["#CLASSES UTILITARIAS"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1706054633046,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"ijOxxDWr2Lqu"},"outputs":[],"source":["class UtilsData():\n","  def __init__(self):\n","    with open('/content/drive/MyDrive/PECE/models/dataset_train_test/dataset_dict.pickle', 'rb') as file:\n","        dataset_dict = pickle.load(file)\n","\n","    self.num_classes = dataset_dict['n_class']\n","    self.dict_classes = {str(k):v for k,v in enumerate(dataset_dict['labels'])}\n","\n","    self.describe_tables = self.load_json(r\"/content/drive/MyDrive/PECE/resoucers/dicionario_dados.json\")\n","    self.intents_table = self.load_json(r\"/content/drive/MyDrive/PECE/resoucers/AdventureQI.json\")\n","    super().__init__()\n","\n","  def load_json(self,file):\n","      f = open(file)\n","      dict = json.load(f)\n","      f.close()\n","      return dict"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"Cfa9WVrr2SXC"},"outputs":[],"source":["class UtilsDB(metaclass = SingletonMeta):\n","  def __init__(self):\n","    self.path_scripts = r\"/content/drive/MyDrive/PECE/scripts\"\n","    self.path_data = r\"/content/drive/MyDrive/PECE/dados\"\n","    self.conn = sqlite3.connect(r\"/content/drive/MyDrive/PECE/db/dbo.db\")\n","\n","    super().__init__()\n","    pass\n","\n","  def consulta_db(self,sql):\n","    try:\n","      df = pd.read_sql(sql,self.conn)\n","      resposta = df.to_dict(\"records\")\n","    except:\n","      resposta = f\"ERRO SQL: {sql}\"\n","    return resposta\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"emIN2K14aEH_"},"outputs":[],"source":["class UtilsChat(UtilsData,UtilsDB):\n","  def __init__(self):\n","    super().__init__()\n","    pass\n","\n","  def captura_consulta(self,lista):\n","      regex = r\"((?:(((`{1,3}|\\\\n)([Ss][Qq][Ll]))|(```))(\\s|\\\\n|```))|(?=[Ss][Ee][Ll][Ee][Cc][Tt]))(.*?)(;|`{1,3}|$)\"\n","      sql_query = lista[0].split(\"###RESPOSTA\")[-1]\n","\n","      for i in re.findall(regex,sql_query, re.DOTALL):\n","        return i[7]\n","\n","  def cria_sql(self,question,table,fields):\n","    return self.chain_llm.invoke({\"question\":question,\"table\":\",\".join(table),\"fields\":fields})\n","\n","  def cria_resposta(self,question,results):\n","    return self.chain_chat.invoke({\"question\":question,\"result\":results})\n","\n","  def interacao_chat(self,question):\n","    intention = self.intent_recognition.identify_intention(question)\n","    intention = self.intent_recognition.identify_intention(question)\n","    intention = self.intent_recognition.identify_intention(question)\n","\n","    table_for_intention = self.intents_table[intention]\n","    columns_for_table = str({table:self.describe_tables[table]['Colunas'] for table in table_for_intention})\n","    sql = self.cria_sql(question,table_for_intention,columns_for_table)\n","\n","    print(\"Resposta Gerativo\",sql)\n","    if isinstance(sql,list):\n","      sql= self.captura_consulta(sql)\n","\n","    if sql == None:\n","        resultado = \"Nenhum\"\n","        resposta = self.cria_resposta(question,\"Não foi possivel localizar\")\n","        print(\"QUESTAO   >>>>> {}\\nINTENÇÃO  >>>>> {} \\nTABELA    >>>>> {} \\nQUERY     >>>>> {} \\nRESULTADO >>>>> {}\".format(question,intention,table_for_intention,sql,str(resultado)))\n","        return resposta\n","\n","    resultado = self.consulta_db(sql.replace(\"\\\\\",\"\"))\n","    resposta = self.cria_resposta(question,str(resultado))\n","    if isinstance(resposta,list):\n","      resposta = resposta[0].split(\"[/INST]\")[-1]\n","\n","    print(\"QUESTAO   >>>>> {}\\nINTENÇÃO  >>>>> {} \\nTABELA    >>>>> {} \\nQUERY     >>>>> {} \\nRESULTADO >>>>> {}\".format(question,intention,table_for_intention,sql,str(resultado)))\n","    return resposta.split(\".\")[0]\n","\n","\n","\n","  def interacao_chat_teste(self,question):\n","    intention1 = self.intent_recognition.identify_intention(question)\n","\n","    dict_saida = dict()\n","    intention = intention1\n","    table_for_intention = self.intents_table[intention]\n","    columns_for_table = str({table:self.describe_tables[table]['Colunas'] for table in table_for_intention})\n","    sql = self.cria_sql(question,table_for_intention,columns_for_table)\n","    resposta_gerada = sql\n","    print(\"Questao\",question)\n","    if isinstance(sql,list):\n","      sql= self.captura_consulta(sql)\n","\n","\n","    if sql == None:\n","        resultado = \"Nenhum\"\n","        resposta = self.cria_resposta(question,\"Não foi possivel localizar\")\n","        # print(\"QUESTAO   >>>>> {}\\nINTENÇÃO  >>>>> {} \\nTABELA    >>>>> {} \\nQUERY     >>>>> {} \\nRESULTADO >>>>> {}\".format(question,intention,table_for_intention,sql,str(resultado)))\n","    else:\n","      resultado = self.consulta_db(sql.replace(\"\\\\\",\"\"))\n","      resposta = self.cria_resposta(question,str(resultado))\n","\n","    if isinstance(resposta,list):\n","        resposta = resposta[0].split(\"[/INST]\")[-1]\n","\n","    # print(\"QUESTAO   >>>>> {}\\nINTENÇÃO  >>>>> {} \\nTABELA    >>>>> {} \\nQUERY     >>>>> {} \\nRESULTADO >>>>> {}\".format(question,intention,table_for_intention,sql,str(resultado)))\n","\n","\n","    dict_saida ={\"Questao\":question,\n","                         \"Intencao\":intention,\n","                         \"Gerativo\":resposta_gerada,\n","                         \"Query Gerada\":sql,\n","                         \"Resultado gerado\":resultado,\n","                         \"Resposta\":resposta.split(\".\")[0]}\n","\n","\n","\n","    return dict_saida\n","\n","\n","\n","  def avaliacao_gerativos(self,query,question,intention):\n","    resultado_query_gerativo = list()\n","    resultado_query_teste = list()\n","    total_questoes = len(question)\n","    for i, quest in enumerate(question):\n","      print(\"{} de {}\".format(i,total_questoes))\n","\n","      table_for_intention = self.intents_table[intention[i]]\n","      columns_for_table = str({table:self.describe_tables[table]['Colunas'] for table in table_for_intention})\n","      sql = self.cria_sql(quest,table_for_intention,columns_for_table)\n","      resposta_gerada = sql\n","      if isinstance(sql,list):\n","        sql= self.captura_consulta(sql)\n","\n","      if sql == None:\n","        resultado = \"Nenhum\"\n","      else:\n","        resultado = self.consulta_db(sql.replace(\"\\\\\",\"\").replace(\"-\",\"_\"))\n","\n","      resultado_query_gerativo.append({ \"Questao\":quest,\n","                                       \"Intencao\":intention[i],\n","                                        \"Query teste\":query[i],\n","                                        \"Gerativo\":resposta_gerada,\n","                                        \"Query Gerada\":sql,\n","                                        \"Resultado teste\":self.consulta_db(query[i].replace(\"\\\\\",\"\")),\n","                                        \"Resultado gerado\":resultado})\n","      # resultado_query_teste.append(self.consulta_db(query[i].replace(\"\\\\\",\"\")))\n","\n","\n","    return resultado_query_gerativo\n","\n","\n","  def interacao_teste(self,question):\n","    intention = self.intent_recognition.identify_intention(question)\n","    table_for_intention = self.intents_table[intention]\n","    columns_for_table = str({table:self.describe_tables[table]['Colunas'] for table in table_for_intention})\n","    sql = self.cria_sql(question,table_for_intention,columns_for_table)\n","\n","    if isinstance(sql,list):\n","      sql= self.captura_consulta(sql)\n","\n","    if sql == None:\n","        resultado = \"Nenhum\"\n","        return intention, resultado\n","\n","    resultado = self.consulta_db(sql.replace(\"\\\\\",\"\"))\n","\n","    return intention, resultado\n"]},{"cell_type":"markdown","metadata":{"id":"5EiaK8rkpXCs"},"source":["#INTENT RECOGINITION"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"OvhGxHm3mHcS"},"outputs":[],"source":["class UtilsBert(UtilsData):\n","  def __init__(self):\n","    self.thresold_multilabel = 0.50\n","    super().__init__()\n","    pass\n","\n","  def tokenization(self,sent):\n","    input_ids,attention_masks=list(),list()\n","    bert_inp=self.tokenizer.encode_plus(sent, add_special_tokens = True, max_length =64, pad_to_max_length = True, return_attention_mask = True)\n","    input_ids.append(bert_inp['input_ids'])\n","    attention_masks.append(bert_inp['attention_mask'])\n","    input_ids=np.asarray(input_ids)\n","    attention_masks=np.array(attention_masks)\n","    return input_ids, attention_masks\n","\n","  def identify_intention(self,sent,response=\"label\"):\n","    input,attention = self.tokenization(sent)\n","    preds = self.model.predict([input,attention],batch_size=1, verbose=0)\n","\n","    if response == \"label\":\n","      return self.label_classification(preds)\n","    else:\n","      return self.multilabel_classification(preds)\n","\n","  def multilabel_classification(self,preds):\n","    labels = list()\n","    idx=np.where(preds.logits>=self.thresold_multilabel,1,0)\n","    for i,col in enumerate(idx[0]):\n","      if col == 1:\n","        labels.append((self.dict_classes[str(i)],preds.logits[0][i]))\n","    return labels\n","\n","  def label_classification(self,preds):\n","    pred_labels = np.argmax(preds.logits, axis=1)\n","    return self.dict_classes[str(pred_labels[0])]"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"RlDEy4FQlkpj"},"outputs":[],"source":["class FineTuningBertimbau(UtilsBert,metaclass=SingletonMeta):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.model_id = 'neuralmind/bert-base-portuguese-cased'\n","    self.weights = '/content/drive/MyDrive/PECE/models/bertimbau_model_v4.h5'\n","\n","    self.tokenizer =  AutoTokenizer.from_pretrained(self.model_id)\n","    self.model = TFBertForSequenceClassification.from_pretrained(self.model_id,num_labels=self.num_classes)\n","    self.model.load_weights(self.weights)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"2j8bR6q6nFnt"},"outputs":[],"source":["class FineTuningRoBERTaXLM(UtilsBert, metaclass=SingletonMeta):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.model_id = \"xlm-roberta-large\"\n","    self.weights = '/content/drive/MyDrive/PECE/models/xlmroberta_model_v5.h5'\n","\n","    self.tokenizer =  AutoTokenizer.from_pretrained(self.model_id)\n","    self.model = TFXLMRobertaForSequenceClassification.from_pretrained(self.model_id, num_labels=self.num_classes,num_hidden_layers =12)\n","    self.model.load_weights(self.weights)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"Pxx4uI2_oxEC"},"outputs":[],"source":["class FineTuningaAlBERTina(UtilsBert, metaclass=SingletonMeta):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.model_id = 'PORTULAN/albertina-900m-portuguese-ptbr-encoder-brwac'\n","    self.weights = '/content/drive/MyDrive/PECE/models/albertina_model_v4.h5'\n","\n","    self.tokenizer =  AutoTokenizer.from_pretrained(self.model_id)\n","    self.model = TFDebertaV2ForSequenceClassification.from_pretrained(self.model_id,num_labels=self.num_classes,num_hidden_layers =12)\n","    self.model.load_weights(self.weights)"]},{"cell_type":"markdown","metadata":{"id":"n5RNLzf2pC8m"},"source":["#AGENTS TEXT-TO-SQL"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"zYg3BmkPkOs1"},"outputs":[],"source":["class ChatMaritalk(SimpleChatModel):\n","\n","    api_key: str\n","    temperature: float = 0.7\n","    chat_mode: bool = True\n","    max_tokens: int = 512\n","    do_sample: bool = True\n","    top_p: float = 0.95\n","    system_message_workaround: bool = True\n","\n","    @property\n","    def _llm_type(self) -> str:\n","        return \"maritalk\"\n","\n","    def parse_messages_for_model(self, messages: List[BaseMessage]):\n","        parsed_messages=[]\n","\n","        for message in messages:\n","            if isinstance(message, HumanMessage):\n","                parsed_messages.append({\"role\":\"user\",\"content\":message.content})\n","            elif isinstance(message, AIMessage):\n","                parsed_messages.append({\"role\":\"assistant\",\"content\":message.content})\n","            elif isinstance(message, SystemMessage):\n","                if self.system_message_workaround:\n","                    parsed_messages.append({\"role\":\"user\",\"content\":message.content})\n","                    parsed_messages.append({\"role\":\"assistant\",\"content\": \"ok\"})\n","\n","        return parsed_messages\n","\n","    def _call(\n","        self,\n","        messages: List[BaseMessage],\n","        stop: Optional[List[str]] = None,\n","        run_manager: Optional[CallbackManagerForLLMRun] = None,\n","        **kwargs: Any,\n","    ) -> str:\n","        print(self.temperature)\n","        model = maritalk.MariTalk(key=self.api_key)\n","        stop_tokens=stop if stop is not None else []\n","        messages=self.parse_messages_for_model(messages)\n","        answer = model.generate(\n","            messages,\n","            temperature=self.temperature,\n","            max_tokens=self.max_tokens,\n","            stopping_tokens=stop_tokens,\n","            do_sample=self.do_sample,\n","            top_p=self.top_p,\n","            chat_mode=True,\n","        )\n","\n","        return answer\n","\n","    @property\n","    def _identifying_params(self) -> Mapping[str, Any]:\n","        \"\"\"Get the identifying parameters.\"\"\"\n","        return {\"system_message_workaround\": self.system_message_workaround, \"temperature\": self.temperature, \"top_p\": self.top_p, \"max_tokens\": self.max_tokens }"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"tlLBr8fglHzg"},"outputs":[],"source":["class AgentMaritaca(UtilsChat):\n","\n","  def __init__(self,model_intent ,**kwargs):\n","    super().__init__()\n","    self.intent_recognition = model_intent()\n","\n","    self.temperature_llm = kwargs.get('temperature_llm',0.7)\n","    self.temperature_chat = kwargs.get('temperature_llm',0.7)\n","\n","    self.API_KEY = \"115462282570231901569$ba212bf19c4a16c8d1fd183e17e64279534d9eaf7fe0c70288c7c0e04324e46c\"\n","    self.llm = ChatMaritalk(api_key=self.API_KEY,chat_mode=False,temperature=self.temperature_llm,max_tokens=100)\n","    self.chat = ChatMaritalk(api_key=self.API_KEY,chat_mode=True,temperature=self.temperature_chat,max_tokens=30)\n","    self.output_parser = StrOutputParser()\n","\n","\n","    self.llm_prompt_template = ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"Você é um assistente especialista em construir instruções SQL. Crie uma query SQL que atenda a solicitação levando em conta as tabelas envolvidas e os campos\"\"\"),\n","        (\"human\", \"\"\"Solicitação: {question}, Tabelas: {table}, Campos:{fields}\"\"\")\n","    ])\n","    self.chat_prompt_template = ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"Você é uma assistente especialista atender solicitações de informações com informações extraidas de banco de dados. . Responda a solicitação com o resultado fornecido.\"\"\"),\n","        (\"human\", \"\"\"Solicitação:{question}, Resultado:{result}\"\"\"),\n","    ])\n","    self.chain_llm = self.llm_prompt_template | self.llm | self.output_parser\n","    self.chain_chat = self.chat_prompt_template | self.chat | self.output_parser\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"ATEKgpZkv7lu"},"outputs":[],"source":["class AgentMaritacaMOD(UtilsChat):\n","\n","  def __init__(self,model_intent1 ,model_intent2 ,model_intent3 ,**kwargs):\n","    super().__init__()\n","    self.intent_recognition1 = model_intent1()\n","    self.intent_recognition2 = model_intent2()\n","    self.intent_recognition3 = model_intent3()\n","\n","    self.temperature_llm = kwargs.get('temperature_llm',0.7)\n","    self.temperature_chat = kwargs.get('temperature_llm',0.7)\n","\n","    self.API_KEY = \"115462282570231901569$ba212bf19c4a16c8d1fd183e17e64279534d9eaf7fe0c70288c7c0e04324e46c\"\n","    self.llm = ChatMaritalk(api_key=self.API_KEY,chat_mode=False,temperature=self.temperature_llm,max_tokens=100)\n","    self.chat = ChatMaritalk(api_key=self.API_KEY,chat_mode=True,temperature=self.temperature_chat,max_tokens=30)\n","    self.output_parser = StrOutputParser()\n","\n","\n","    self.llm_prompt_template = ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"Você é um assistente especialista em construir instruções SQL. Crie uma query SQL que atenda a solicitação levando em conta as tabelas envolvidas e os campos\"\"\"),\n","        (\"human\", \"\"\"Solicitação: {question}, Tabelas: {table}, Campos:{fields}\"\"\")\n","    ])\n","    self.chat_prompt_template = ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"Você é uma assistente especialista atender solicitações de informações com informações extraidas de banco de dados. Responda a solicitação EM PORTUGUÊS com o resultado fornecido.\"\"\"),\n","        (\"human\", \"\"\"Solicitação:{question}, Resultado:{result}\"\"\"),\n","    ])\n","    self.chain_llm = self.llm_prompt_template | self.llm | self.output_parser\n","    self.chain_chat = self.chat_prompt_template | self.chat | self.output_parser"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"S4AtSkctxZBk"},"outputs":[],"source":["class BaseModelMixtral(metaclass = SingletonMeta):\n","\n","    def __init__(self, **kwargs: Any):\n","        model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n","        quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n","        state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n","\n","        config = AutoConfig.from_pretrained(quantized_model_name)\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name,padding_side='left')\n","        device = torch.device(\"cuda:0\")\n","        offload_per_layer = 1\n","\n","        num_experts = config.num_local_experts\n","\n","        offload_config = OffloadConfig(\n","            main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n","            offload_size=config.num_hidden_layers * offload_per_layer,\n","            buffer_size=4,\n","            offload_per_layer=offload_per_layer,\n","        )\n","\n","\n","        attn_config = BaseQuantizeConfig(\n","            nbits=4,\n","            group_size=64,\n","            quant_zero=True,\n","            quant_scale=True,\n","        )\n","        attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n","\n","\n","        ffn_config = BaseQuantizeConfig(\n","            nbits=2,\n","            group_size=16,\n","            quant_zero=True,\n","            quant_scale=True,\n","        )\n","        quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n","\n","\n","        self.model = build_model(\n","            device=device,\n","            quant_config=quant_config,\n","            offload_config=offload_config,\n","            state_path=state_path,\n","        )\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"6j2IS2tPauuf"},"outputs":[],"source":["class ChatMixtral(SimpleChatModel):\n","\n","    temperature: float = 0.7\n","    max_tokens: int = 512\n","    do_sample: bool = True\n","    top_p: float = 0.95\n","    system_message_workaround: bool = True\n","    device = torch.device(\"cuda:0\")\n","\n","    @property\n","    def _llm_type(self) -> str:\n","      return \"mixtral\"\n","\n","    def __init__(self,**kwargs):\n","      super().__init__()\n","      self.temperature = kwargs.get(\"temperature\",self.temperature)\n","      self.max_tokens = kwargs.get(\"max_tokens\",self.max_tokens)\n","      self.do_sample = kwargs.get(\"do_sample\",self.do_sample)\n","      self.top_p = kwargs.get(\"top_p\",self.top_p)\n","      self.system_message_workaround = kwargs.get(\"system_message_workaround\",self.system_message_workaround)\n","\n","    def parse_messages_for_model(self, messages: List[BaseMessage]):\n","        parsed_messages=[]\n","\n","        for message in messages:\n","            # print(message,isinstance(message, AIMessage))\n","            if isinstance(message, HumanMessage):\n","                parsed_messages.append({\"role\":\"user\",\"content\":message.content})\n","            elif isinstance(message, AIMessage):\n","                parsed_messages.append({\"role\":\"assistant\",\"content\":message.content})\n","            elif isinstance(message, SystemMessage):\n","                if self.system_message_workaround:\n","                    parsed_messages.append({\"role\":\"user\",\"content\":message.content})\n","                    parsed_messages.append({\"role\":\"assistant\",\"content\": \"ok\"})\n","\n","        return parsed_messages\n","\n","    def _call(\n","        self,\n","        messages: List[BaseMessage],\n","        stop: Optional[List[str]] = None,\n","        run_manager: Optional[CallbackManagerForLLMRun] = None,\n","        **kwargs: Any,\n","    ) -> str:\n","        print\n","        messages=self.parse_messages_for_model(messages)\n","        mixtral = BaseModelMixtral()\n","        input_ids = mixtral.tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(self.device)\n","        attention_mask = torch.ones_like(input_ids)\n","        answer = mixtral.model.generate(\n","              input_ids=input_ids,\n","              attention_mask=attention_mask,\n","              do_sample=self.do_sample,\n","              temperature=self.temperature,\n","              top_p=self.top_p,\n","              max_new_tokens=self.max_tokens,\n","              pad_token_id=mixtral.tokenizer.eos_token_id\n","              )\n","\n","\n","        decoded = mixtral.tokenizer.batch_decode(answer, skip_special_tokens=True)\n","        return decoded\n","\n","\n","    @property\n","    def _identifying_params(self) -> Mapping[str, Any]:\n","      \"\"\"Get the identifying parameters.\"\"\"\n","      return {\"system_message_workaround\": self.system_message_workaround, \"temperature\": self.temperature, \"top_p\": self.top_p, \"max_tokens\": self.max_tokens }\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"SG3EA1hxiZFV"},"outputs":[],"source":["class AgentMixtral(UtilsChat):\n","\n","  def __call__(self,**kwargs):\n","    self.temperature_llm = kwargs.get('temperature_llm',0.4)\n","    self.temperature_chat = kwargs.get('temperature_llm',0.7)\n","\n","  def __init__(self,model_intent ,**kwargs):\n","    super().__init__()\n","    self.intent_recognition = model_intent()\n","    self.temperature_llm = kwargs.get('temperature_llm',0.4)\n","    self.temperature_chat = kwargs.get('temperature_llm',0.7)\n","\n","    print(\"TEMPERATURA\",self.temperature_llm)\n","\n","    self.llm = ChatMixtral(temperature=self.temperature_llm,max_tokens=500)\n","    self.chat = ChatMixtral(temperature=self.temperature_chat,max_tokens=50,)\n","    self.output_parser = StrOutputParser()\n","\n","\n","    self.llm_prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"Você é um assistente especialista em construir instruções SQL. Crie uma query SQL no SQLITE que atenda a solicitação levando em conta SOMENTE as tabelas e campos enviados\"\"\"),\n","        (\"human\", \"\"\"###Solicitação: {question} ###Tabelas: {table} ###Campos:{fields}\"\"\"),\n","        (\"ai\", \"\"\"###RESPOSTA: \"\"\")])\n","\n","    self.chat_prompt =ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"\"Você é um assistente especialista em atender solicitações com dados extraidos de bancos de dados. Responda a solicitação EM PORTUGUÊS com o resultado fornecido.\"\"\"),\n","        (\"human\", \"\"\"###SOLICITAÇÃO:{question} ###RESULTADO:{result}\"\"\")])\n","\n","\n","    self.chain_llm = self.llm_prompt | self.llm | self.output_parser\n","    self.chain_chat = self.chat_prompt | self.chat | self.output_parser"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1706054633047,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"j7XwhUAKXvod"},"outputs":[],"source":["class AgentMixtralMOD(UtilsChat):\n","\n","  def __call__(self,**kwargs):\n","    self.temperature_llm = kwargs.get('temperature_llm',0.4)\n","    self.temperature_chat = kwargs.get('temperature_llm',0.7)\n","\n","  def __init__(self,model_intent1 ,model_intent2,model_intent3,**kwargs):\n","    super().__init__()\n","    self.intent_recognition1 = model_intent1()\n","    self.intent_recognition2 = model_intent2()\n","    self.intent_recognition3 = model_intent3()\n","    self.temperature_llm = kwargs.get('temperature_llm',0.4)\n","    self.temperature_chat = kwargs.get('temperature_llm',0.7)\n","\n","    print(\"TEMPERATURA\",self.temperature_llm)\n","\n","    self.llm = ChatMixtral(temperature=self.temperature_llm,max_tokens=500)\n","    self.chat = ChatMixtral(temperature=self.temperature_chat,max_tokens=50,)\n","    self.output_parser = StrOutputParser()\n","\n","\n","    self.llm_prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"Você é um assistente especialista em construir instruções SQL. Crie uma query SQL que atenda a solicitação levando em conta tabelas e campos enviados\"\"\"),\n","        (\"human\", \"\"\"###Solicitação: {question} ###Tabelas: {table} ###Campos:{fields}\"\"\"),\n","        (\"ai\", \"\"\"###RESPOSTA: \"\"\")])\n","\n","    self.chat_prompt =ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"\"Você é um assistente especialista em atender solicitações com dados extraidos de bancos de dados. Responda a solicitação EM PORTUGUÊS com o resultado fornecido.\"\"\"),\n","        (\"human\", \"\"\"###SOLICITAÇÃO:{question} ###RESULTADO:{result}\"\"\")])\n","\n","\n","    self.chain_llm = self.llm_prompt | self.llm | self.output_parser\n","    self.chain_chat = self.chat_prompt | self.chat | self.output_parser"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":1217,"status":"ok","timestamp":1706054634254,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"5RaZetFFsoOl"},"outputs":[],"source":["import pickle\n","with open('/content/drive/MyDrive/PECE/models/dataset_train_test/dataset_dict.pickle', 'rb') as file:\n","    dataset_dict = pickle.load(file)\n","\n","num_classes = dataset_dict['n_class']\n","labels = dataset_dict['labels']\n","\n","df = pd.read_excel(\"/content/drive/MyDrive/PECE/resoucers/querys.xlsx\",sheet_name='Planilha1')\n","df_to_use = df.dropna()\n","data=df_to_use.rename(columns = {'NEW INTENT': 'label', 'Question': 'text'}, inplace = False)\n","\n","df_questoes = pd.DataFrame(dataset_dict['X_test'].values,columns=['Questao'])\n","querys = df_questoes.merge(data,left_on=['Questao'],right_on=['text'],how='left')['SQL'].to_list()\n","questoes = dataset_dict['X_test'].values\n","intencoes = [labels[label_index] for label_index in dataset_dict['y_test'].tolist()]"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1706054634254,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"9oTAxjkKN63F"},"outputs":[],"source":["# agent = AgentMixtral(FineTuningBertimbau,temperature_llm=0.4)\n","# mixtral_2 = agent.avaliacao_gerativos(querys,questoes,intencoes)\n","\n","# with open('/content/drive/MyDrive/PECE/models/mixtral_2', 'wb') as fp:\n","#   pickle.dump(mixtral_2, fp)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q2efDiqnpRIb"},"source":["#USO"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706054634254,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"NW0XSJMSpkdP"},"outputs":[],"source":["# question = \"Ocupacao do lucas piazzon\""]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706054634254,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"-BfQMYhNeZvg"},"outputs":[],"source":["# bertimbau = FineTuningBertimbau()\n","\n","# print(bertimbau.identify_intention(question))\n","# print(bertimbau.identify_intention(question,response=\"multilabel\"))"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706054634254,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"GPwSQgNVbyds"},"outputs":[],"source":["# mixtralAgent = AgentMixtral(FineTuningBertimbau,temperature_llm=0.4)\n","# maritacaAgent = AgentMaritaca(FineTuningBertimbau,temperature_llm=0.3)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706054634254,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"LOT0HF-8YN1j"},"outputs":[],"source":["questions = [\"Cargo de Susan Eaton\",\n","             \"Em que cargo Suchitra Mohan esta?\",\n","             \"Christopher Hill é de que departamento?\",\n","             \"De quanto é a renda anual do cliente Arthur Sanchez\",\n","             \"Quantos departamentos existem na empresa?\",\n","            \"Qual o cargo do Paulo Jorge?\",\n","            \"Em qual departamento o Joao Castilho está?\",\n","            \"De quanto é a renda anual do cliente Alex Fernando\",\n","            \"Quantidade de carros do Juliana Ahuig\",\n","            \"Renda Anual cliente Carina Ramos\"\n","             ]"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5919,"status":"ok","timestamp":1706054640165,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"advS4rQxbq_A","outputId":"ef723f94-7280-4ad2-c576-8484c3aa45bc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier', 'bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["TEMPERATURA 0.7\n"]}],"source":["mixtralAgentMOD = AgentMixtral(FineTuningBertimbau,\n","                                  temperature_llm=0.7)\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706055472681,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"Zv3odPYh1EJn"},"outputs":[],"source":["maritacaAgent =  AgentMaritaca(FineTuningBertimbau,\n","                                  temperature_llm=0.7)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45029,"status":"ok","timestamp":1706055517708,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"IOQi62wnaL_D","outputId":"60901a8c-a881-4e41-f00d-1fe3fdc85573"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao Cargo de Susan Eaton\n","0.7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao Em que cargo Suchitra Mohan esta?\n","0.7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao Christopher Hill é de que departamento?\n","0.7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao De quanto é a renda anual do cliente Arthur Sanchez\n","0.7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao Quantos departamentos existem na empresa?\n","0.7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao Qual o cargo do Paulo Jorge?\n","0.7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao Em qual departamento o Joao Castilho está?\n","0.7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao De quanto é a renda anual do cliente Alex Fernando\n","0.7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao Quantidade de carros do Juliana Ahuig\n","0.7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["0.7\n","Questao Renda Anual cliente Carina Ramos\n","0.7\n"]}],"source":["saidas = list()\n","for quest in questions:\n","  saidas.append(maritacaAgent.interacao_chat_teste(quest))"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":332,"status":"ok","timestamp":1706055548281,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"W49JLngsk7Zq","outputId":"1899fd43-b79b-4330-c432-9c8a4b46b97c"},"outputs":[{"data":{"text/plain":["[{'Questao': 'Cargo de Susan Eaton',\n","  'Intencao': 'CARGO_DO_FUNCIONARIO',\n","  'Gerativo': \" SELECT Title\\nFROM DimEmployee\\nWHERE EmployeeKey = 'SUSAN\\\\_EATON\\\\_EMPLOYEE\\\\_KEY'\",\n","  'Query Gerada': \" SELECT Title\\nFROM DimEmployee\\nWHERE EmployeeKey = 'SUSAN\\\\_EATON\\\\_EMPLOYEE\\\\_KEY'\",\n","  'Resultado gerado': [],\n","  'Resposta': ' Desculpe, não tenho informações sobre o cargo de Susan Eaton'},\n"," {'Questao': 'Em que cargo Suchitra Mohan esta?',\n","  'Intencao': 'CARGO_DO_FUNCIONARIO',\n","  'Gerativo': \" SELECT Title FROM DimEmployee WHERE FirstName = 'Suchitra' AND LastName = 'Mohan';\",\n","  'Query Gerada': \" SELECT Title FROM DimEmployee WHERE FirstName = 'Suchitra' AND LastName = 'Mohan';\",\n","  'Resultado gerado': [{'Title': 'Production Technician - WC60'}],\n","  'Resposta': 'According to the information I have, Suchitra Mohan is currently a Production Technician - WC60'},\n"," {'Questao': 'Christopher Hill é de que departamento?',\n","  'Intencao': 'DEPARTAMENTO_DO_FUNCIONARIO',\n","  'Gerativo': \" SELECT DepartmentName\\nFROM DimEmployee\\nWHERE FirstName = 'Christopher' AND LastName = 'Hill';\",\n","  'Query Gerada': \" SELECT DepartmentName\\nFROM DimEmployee\\nWHERE FirstName = 'Christopher' AND LastName = 'Hill';\",\n","  'Resultado gerado': [{'DepartmentName': 'Production'}],\n","  'Resposta': ' Christopher Hill é do departamento de Produção'},\n"," {'Questao': 'De quanto é a renda anual do cliente Arthur Sanchez',\n","  'Intencao': 'INFORMACAO_PESSOAL_CLIENTE',\n","  'Gerativo': \" SELECT YEARLYINCOME FROM DimCustomer WHERE FIRSTNAME = 'Arthur' AND LASTNAME = 'Sanchez';\",\n","  'Query Gerada': \" SELECT YEARLYINCOME FROM DimCustomer WHERE FIRSTNAME = 'Arthur' AND LASTNAME = 'Sanchez';\",\n","  'Resultado gerado': [{'YearlyIncome': 20000.0}],\n","  'Resposta': ' Não tenho informações específicas sobre o cliente Arthur Sanchez, pois não tenho acesso a'},\n"," {'Questao': 'Quantos departamentos existem na empresa?',\n","  'Intencao': 'QUANTIDADE_DE_DEPARTAMENTOS',\n","  'Gerativo': ' SELECT COUNT(DISTINCT DepartmentName) FROM DimEmployee;',\n","  'Query Gerada': ' SELECT COUNT(DISTINCT DepartmentName) FROM DimEmployee;',\n","  'Resultado gerado': [{'COUNT(DISTINCT DepartmentName)': 16}],\n","  'Resposta': ' Isso significa que existem 16 departamentos na empresa'},\n"," {'Questao': 'Qual o cargo do Paulo Jorge?',\n","  'Intencao': 'CARGO_DO_FUNCIONARIO',\n","  'Gerativo': \" SELECT Title\\nFROM DimEmployee\\nWHERE FirstName = 'Paulo' AND LastName = 'Jorge';\",\n","  'Query Gerada': \" SELECT Title\\nFROM DimEmployee\\nWHERE FirstName = 'Paulo' AND LastName = 'Jorge';\",\n","  'Resultado gerado': [],\n","  'Resposta': ' Não encontrei informações sobre o cargo do Paulo Jorge'},\n"," {'Questao': 'Em qual departamento o Joao Castilho está?',\n","  'Intencao': 'DEPARTAMENTO_DO_FUNCIONARIO',\n","  'Gerativo': \" SELECT DepartmentName\\nFROM DimEmployee\\nWHERE FirstName = 'Joao' AND LastName = 'Castilho';\",\n","  'Query Gerada': \" SELECT DepartmentName\\nFROM DimEmployee\\nWHERE FirstName = 'Joao' AND LastName = 'Castilho';\",\n","  'Resultado gerado': [],\n","  'Resposta': 'Desculpe, não tenho essa informação pois não tenho acesso a banco de dados espec'},\n"," {'Questao': 'De quanto é a renda anual do cliente Alex Fernando',\n","  'Intencao': 'INFORMACAO_PESSOAL_CLIENTE',\n","  'Gerativo': \" SELECT YEARLYINCOME\\nFROM DIMCUSTOMER\\nWHERE FIRSTNAME = 'Alex'\\nAND LASTNAME = 'Fernando';\",\n","  'Query Gerada': \" SELECT YEARLYINCOME\\nFROM DIMCUSTOMER\\nWHERE FIRSTNAME = 'Alex'\\nAND LASTNAME = 'Fernando';\",\n","  'Resultado gerado': [],\n","  'Resposta': ' Desculpe, mas como uma IA não tenho acesso a informações pessoais de indivíduos, incl'},\n"," {'Questao': 'Quantidade de carros do Juliana Ahuig',\n","  'Intencao': 'INFORMACAO_PESSOAL_CLIENTE',\n","  'Gerativo': \" SELECT COUNT(\\\\*)\\nFROM DimCustomer\\nWHERE FirstName = 'Juliana'\\nAND LastName = 'Ahuig'\",\n","  'Query Gerada': \" SELECT COUNT(\\\\*)\\nFROM DimCustomer\\nWHERE FirstName = 'Juliana'\\nAND LastName = 'Ahuig'\",\n","  'Resultado gerado': [{'COUNT(*)': 0}],\n","  'Resposta': ' Isso significa que não há registros de carros em um banco de dados que correspondam a Juliana Ahuig'},\n"," {'Questao': 'Renda Anual cliente Carina Ramos',\n","  'Intencao': 'INFORMACAO_PESSOAL_CLIENTE',\n","  'Gerativo': \" SELECT YEARLYINCOME FROM DIMCUSTOMER WHERE FIRSTNAME='CARINA' AND LASTNAME='RAMOS';\",\n","  'Query Gerada': \" SELECT YEARLYINCOME FROM DIMCUSTOMER WHERE FIRSTNAME='CARINA' AND LASTNAME='RAMOS';\",\n","  'Resultado gerado': [],\n","  'Resposta': ' Desculpe, mas como uma IA, eu não tenho acesso a informações pessoais de indivíduos'}]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["saidas"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1706055288051,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"vSqROh4HoRIB","outputId":"87f95bc1-e0ff-42cd-c6e9-33011f7bd9f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["SELECT Title \n","FROM DimEmployee \n","WHERE FirstName = 'Paulo' AND LastName = 'Jorge'\n","\n","SELECT` pode retornar zero, um ou múltiplos registros, dependendo do número de ocorrências da cláusula `WHERE`. No caso de \"Paulo Jorge\" ser um funcionário único, a instrução retornará apenas uma linha com o seu respectivo cargo.\n"]}],"source":["regex = r\"((?:(((```|\\\\n)([Ss][Qq][Ll]))|(```))(\\s|\\\\n|```))|(?=[Ss][Ee][Ll][Ee][Cc][Tt]))(.*?)(;|```|$)\"\n","sql_query = saidas[5][\"Gerativo\"][0].split(\"###RESPOSTA\")[-1]\n","\n","for i in re.findall(regex,sql_query, re.DOTALL):\n","  print(i[7])"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":568,"status":"ok","timestamp":1706055420133,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"RGio90rv03EJ"},"outputs":[],"source":["with open('/content/drive/MyDrive/PECE/models/geracao_resposta_maritaca', 'wb') as fp:\n","  pickle.dump(saidas, fp)\n"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706055288051,"user":{"displayName":"Rogério Piazzon Teixeira Junior","userId":"15623116211726532081"},"user_tz":180},"id":"EVU7fpb5c4T2"},"outputs":[],"source":["# from google.colab import runtime\n","# runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNJwxe6VUAlUTCzcDn/bt+p","gpuType":"A100","machine_shape":"hm","mount_file_id":"13HVfDDsTI1FoIONq6qFopNKt6_Cp_wMD","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
